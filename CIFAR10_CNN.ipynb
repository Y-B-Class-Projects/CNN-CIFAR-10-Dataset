{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "CIFAR10_CNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Y-B-Class-Projects/CNN-CIFAR-10-Dataset/blob/main/CIFAR10_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GTgBvtcRD-q"
      },
      "source": [
        "### Written by: Baruch Baksht: 211302088, Israel Rolnik: 206672057\n",
        "### Lecturer: professor Avi Rosenfeld"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIxsHxttP2k5"
      },
      "source": [
        "## Building a CNN to classify images in the CIFAR-10 Dataset\n",
        "\n",
        "We will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The 10 classes are:\n",
        "\n",
        "<ol start=\"0\">\n",
        "<li> airplane\n",
        "<li>  automobile\n",
        "<li> bird\n",
        "<li>  cat\n",
        "<li> deer\n",
        "<li> dog\n",
        "<li>  frog\n",
        "<li>  horse\n",
        "<li>  ship\n",
        "<li>  truck\n",
        "</ol>\n",
        "\n",
        "For details about CIFAR-10 see:\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "For a compilation of published performance results on CIFAR 10, see:\n",
        "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
        "\n",
        "---\n",
        "\n",
        "### Building Convolutional Neural Nets\n",
        "\n",
        "In this exercise we will build and train our first convolutional neural networks.  In the first part, we walk through the different layers and how they are configured.  In the second part, you will build your own model, train it, and compare the performance.\n",
        "\n",
        "Much of this code is from Intel's course at: https://software.intel.com/content/dam/develop/public/us/en/downloads/intel-dl101-class6.zip, but I added some tweaks of my own ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQC8thNfP2k_"
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfQekVzZP2lA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ca0247-c9f4-46e7-d8e4-10f31223c515"
      },
      "source": [
        "# The data, shuffled and split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nc4W3uKIP2lA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b029050-3c82-47e9-ef98-dfefdf533eb8"
      },
      "source": [
        "## Each image is a 32 x 32 x 3 numpy array\n",
        "x_train[444].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePODxy5uP2lA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "1e0478f8-4be7-421a-907a-991313b2d23c"
      },
      "source": [
        "## Let's look at one of the images\n",
        "\n",
        "print(y_train[444])\n",
        "plt.imshow(x_train[444]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "czYvqoRAP2lA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538b06bd-c4f2-4f27-ae03-22bbdae5afdb"
      },
      "source": [
        "num_classes = 10\n",
        "print(y_train[444])\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srcBlBlVP2lB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c66537af-8cf2-497a-ae97-f23268628baa"
      },
      "source": [
        "# now instead of classes described by an integer between 0-9 we have a vector with a 1 in the (Pythonic) 9th position\n",
        "y_train[444]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wSh-NIvhP2lB"
      },
      "source": [
        "# As before, let's make everything float and scale\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd7SfmLsP2lC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8f43a2-5bd2-475a-c2e0-ce722ed8cb0c"
      },
      "source": [
        "# Let's build a CNN using Keras' Sequential capabilities\n",
        "\n",
        "model_1 = Sequential()\n",
        "\n",
        "\n",
        "## 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model_1.add(Activation('relu'))\n",
        "\n",
        "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_1.add(Conv2D(32, (5, 5), strides = (2,2))) \n",
        "model_1.add(Activation('relu'))\n",
        "\n",
        "## 2x2 max pooling reduces to 3 x 3 x 32\n",
        "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_1.add(Dropout(0.25))\n",
        "\n",
        "## Flatten turns 3x3x32 into 288x1\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(512))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Dropout(0.5))\n",
        "model_1.add(Dense(num_classes))\n",
        "model_1.add(Activation('softmax'))\n",
        "\n",
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 16, 16, 32)        2432      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 6, 6, 32)          25632     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               147968    \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 181,162\n",
            "Trainable params: 181,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jmOeUBmvP2lE"
      },
      "source": [
        "### Exercise\n",
        "There are many things we can change in CNNs:\n",
        "1. The batch size\n",
        "2. The parameters within a specific CNN layer such as if padding is used, the size (kernel size) and number of filters used, and stride size\n",
        "3. The Dropout rate (if any) that is used\n",
        "4. The CNN architecture including the number of layers and if and when to use MaxPool (AvgPool usually isn't used today).\n",
        "5. The optimizer used (and the learning rate in the optimizer)\n",
        "6. In theory which activation function, but in CNN's Relu is almost always used except for the last layer which is Softmax (which activation function is Softmax???)\n",
        "\n",
        "Questions: \n",
        "\n",
        "A. Build model_2 which checks the performance of batch sizes of 4, 128 and 1024.  Which one works best after 10 epochs?  Which one runs the fastest?\n",
        "\n",
        "B. Build model_3 which adds padding to all layers. Does that improve performance?\n",
        "\n",
        "C. Build model_4 which uses the same architecture from the MNIST network. Does that work better?\n",
        "\n",
        "D. Build model_5 which adds to the structure of model_1-- either by adding more convolution layers or Maxpool layers.   Intel suggests trying the architecture: \n",
        "\n",
        "Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
        "\n",
        "instead of:\n",
        "\n",
        "Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
        "\n",
        "If you do this, you will likely need to lower the stride to 1 as otherwise the layers are not big enough in this dataset!\n",
        "\n",
        "Please run the network that worked the best for more than 10 epochs and see how good you get!\n",
        "\n",
        "Hint:  Feel free to work on different colab notebooks in parallel (Google doesn't care until around 5 at the same time) and / or run the code on your computer and then paste it back to Colab when you are done (the code works faster on my computer than in Colab but everyone's computer is different).\n",
        "\n",
        "Your grade is based on the following:\n",
        "\n",
        "80 points for correctly doing all questions and documenting your solution including the answers to the my questions.\n",
        "\n",
        "5 points for breaking 65% accuracy in your best model\n",
        "\n",
        "10 points for breaking 70% accuracy in your best model\n",
        "\n",
        "3 points for breaking 72% accuracy in your best model\n",
        "\n",
        "2 points for breaking 75% accuracy in your best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEe7lvZPyrty"
      },
      "source": [
        "\n",
        "---\n",
        "# Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3QLXnGgyd79"
      },
      "source": [
        "### model_2 \n",
        "(which checks the performance of batch sizes of 4, 128 and 1024)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2DVRwD0zds6",
        "outputId": "ffe5bb7d-b9c6-4bb9-ad4a-680bc5cfd428"
      },
      "source": [
        "model_2 = Sequential()\n",
        "\n",
        "## 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_2.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model_2.add(Activation('relu'))\n",
        "\n",
        "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_2.add(Conv2D(32, (5, 5), strides = (2,2))) \n",
        "model_2.add(Activation('relu'))\n",
        "\n",
        "## 2x2 max pooling reduces to 3 x 3 x 32\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_2.add(Dropout(0.25))\n",
        "\n",
        "## Flatten turns 3x3x32 into 288x1\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(512))\n",
        "model_2.add(Activation('relu'))\n",
        "model_2.add(Dropout(0.5))\n",
        "model_2.add(Dense(num_classes))\n",
        "model_2.add(Activation('softmax'))\n",
        "\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 32)        2432      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 6, 32)          25632     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               147968    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 181,162\n",
            "Trainable params: 181,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXR8S7ss1HYW"
      },
      "source": [
        "### batch_size = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbpFLKXG1GyY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb6fbfe-0fa4-467a-e3b0-e89c0a6cbbca"
      },
      "source": [
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "\n",
        "# initiate Adam optimizer like in the first example\n",
        "model_2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_2.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=num_epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "score = model_2.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "12500/12500 [==============================] - 67s 3ms/step - loss: 1.8817 - accuracy: 0.3042 - val_loss: 1.4600 - val_accuracy: 0.4536\n",
            "Epoch 2/10\n",
            "12500/12500 [==============================] - 34s 3ms/step - loss: 1.5452 - accuracy: 0.4381 - val_loss: 1.4813 - val_accuracy: 0.4682\n",
            "Epoch 3/10\n",
            "12500/12500 [==============================] - 33s 3ms/step - loss: 1.4953 - accuracy: 0.4608 - val_loss: 1.3457 - val_accuracy: 0.5229\n",
            "Epoch 4/10\n",
            "12500/12500 [==============================] - 33s 3ms/step - loss: 1.4668 - accuracy: 0.4743 - val_loss: 1.3641 - val_accuracy: 0.5145\n",
            "Epoch 5/10\n",
            "12500/12500 [==============================] - 34s 3ms/step - loss: 1.4476 - accuracy: 0.4838 - val_loss: 1.3702 - val_accuracy: 0.5132\n",
            "Epoch 6/10\n",
            "12500/12500 [==============================] - 34s 3ms/step - loss: 1.4432 - accuracy: 0.4877 - val_loss: 1.3022 - val_accuracy: 0.5267\n",
            "Epoch 7/10\n",
            "12500/12500 [==============================] - 33s 3ms/step - loss: 1.4487 - accuracy: 0.4842 - val_loss: 1.3613 - val_accuracy: 0.5172\n",
            "Epoch 8/10\n",
            "12500/12500 [==============================] - 33s 3ms/step - loss: 1.4327 - accuracy: 0.4945 - val_loss: 1.3086 - val_accuracy: 0.5397\n",
            "Epoch 9/10\n",
            "12500/12500 [==============================] - 34s 3ms/step - loss: 1.4122 - accuracy: 0.4977 - val_loss: 1.2951 - val_accuracy: 0.5464\n",
            "Epoch 10/10\n",
            "12500/12500 [==============================] - 33s 3ms/step - loss: 1.4148 - accuracy: 0.4983 - val_loss: 1.3519 - val_accuracy: 0.5218\n",
            "Test loss: 1.3519376516342163\n",
            "Test accuracy: 0.5217999815940857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv3164XG1a3l"
      },
      "source": [
        "### batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDhhogy01a3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88879498-255d-4d87-d11c-3fd6ab8226aa"
      },
      "source": [
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "\n",
        "# initiate Adam optimizer like in the first example\n",
        "model_2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_2.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=num_epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "score = model_2.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 3s 7ms/step - loss: 1.3063 - accuracy: 0.5374 - val_loss: 1.2061 - val_accuracy: 0.5806\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.2351 - accuracy: 0.5643 - val_loss: 1.1767 - val_accuracy: 0.5877\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.2063 - accuracy: 0.5748 - val_loss: 1.1837 - val_accuracy: 0.5888\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1908 - accuracy: 0.5768 - val_loss: 1.1497 - val_accuracy: 0.5962\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1791 - accuracy: 0.5814 - val_loss: 1.1508 - val_accuracy: 0.5962\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1664 - accuracy: 0.5827 - val_loss: 1.1404 - val_accuracy: 0.6050\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1575 - accuracy: 0.5883 - val_loss: 1.1385 - val_accuracy: 0.6005\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1434 - accuracy: 0.5947 - val_loss: 1.1315 - val_accuracy: 0.6066\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1323 - accuracy: 0.6021 - val_loss: 1.1204 - val_accuracy: 0.6087\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 2s 6ms/step - loss: 1.1256 - accuracy: 0.6024 - val_loss: 1.1292 - val_accuracy: 0.6082\n",
            "Test loss: 1.1292049884796143\n",
            "Test accuracy: 0.6082000136375427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm3J19op1fQO"
      },
      "source": [
        "### batch_size = 1024"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGlbjGdX1fQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b889d381-ab5b-4eed-aa9f-00033ba5a926"
      },
      "source": [
        "batch_size = 1024\n",
        "num_epochs = 10\n",
        "\n",
        "# initiate Adam optimizer like in the first example\n",
        "model_2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_2.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=num_epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "score = model_2.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 2s 28ms/step - loss: 1.0643 - accuracy: 0.6209 - val_loss: 1.0774 - val_accuracy: 0.6251\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 1s 22ms/step - loss: 1.0570 - accuracy: 0.6194 - val_loss: 1.0764 - val_accuracy: 0.6254\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 1s 22ms/step - loss: 1.0451 - accuracy: 0.6266 - val_loss: 1.0743 - val_accuracy: 0.6272\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 1s 22ms/step - loss: 1.0516 - accuracy: 0.6232 - val_loss: 1.0731 - val_accuracy: 0.6275\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 1s 22ms/step - loss: 1.0451 - accuracy: 0.6277 - val_loss: 1.0736 - val_accuracy: 0.6280\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 1.0497 - accuracy: 0.6241 - val_loss: 1.0767 - val_accuracy: 0.6273\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 1.0402 - accuracy: 0.6277 - val_loss: 1.0706 - val_accuracy: 0.6289\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 1s 23ms/step - loss: 1.0405 - accuracy: 0.6285 - val_loss: 1.0715 - val_accuracy: 0.6292\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 1s 22ms/step - loss: 1.0402 - accuracy: 0.6286 - val_loss: 1.0665 - val_accuracy: 0.6298\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 1s 22ms/step - loss: 1.0248 - accuracy: 0.6329 - val_loss: 1.0673 - val_accuracy: 0.6321\n",
            "Test loss: 1.067264199256897\n",
            "Test accuracy: 0.632099986076355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU1ssVG4Aldz"
      },
      "source": [
        "### In conclusion:\n",
        "\n",
        "\n",
        "batch_size = 4 -> accuracy = **52.18%**\n",
        "\n",
        "batch_size = 128 -> accuracy = **60.82%**\n",
        "\n",
        "batch_size = 1024 -> accuracy = **63.21%**\n",
        "\n",
        "\n",
        "(increasing the batch_size increases the accuracy)\n",
        "\n",
        "**Q:** Which one works best after 10 epochs? Which one runs the fastest?\n",
        "\n",
        "**A:** 1024 batch_size works best and fastest.\n",
        "\n",
        "Based on these results we will work from now on with 1024 batch_size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-yiNZtDRKY"
      },
      "source": [
        "batch_size = 1024\n",
        "num_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5A9Cyi4-fyT"
      },
      "source": [
        "### model_3\n",
        "(which adds padding to all layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtkmG_bf-eZg",
        "outputId": "c0fb4f2b-e5c1-474d-c7c0-6a370128720a"
      },
      "source": [
        "model_3 = Sequential()\n",
        "\n",
        "## 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_3.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model_3.add(Activation('relu'))\n",
        "\n",
        "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_3.add(Conv2D(32, (5, 5), strides = (2,2), padding='same')) \n",
        "model_3.add(Activation('relu'))\n",
        "\n",
        "## 2x2 max pooling reduces to 3 x 3 x 32\n",
        "model_3.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_3.add(Dropout(0.25))\n",
        "\n",
        "## Flatten turns 3x3x32 into 288x1\n",
        "model_3.add(Flatten())\n",
        "model_3.add(Dense(512))\n",
        "model_3.add(Activation('relu'))\n",
        "model_3.add(Dropout(0.5))\n",
        "model_3.add(Dense(num_classes))\n",
        "model_3.add(Activation('softmax'))\n",
        "\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 32)        2432      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 6, 6, 32)          25632     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 6, 6, 32)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 3, 3, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               147968    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 181,162\n",
            "Trainable params: 181,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG6QqtVEC8xG",
        "outputId": "42ba324f-219a-460e-fc17-6042a5f72aa1"
      },
      "source": [
        "# initiate Adam optimizer like in the first example\n",
        "model_3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_3.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=num_epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "score = model_3.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 2s 32ms/step - loss: 1.1181 - accuracy: 0.6098 - val_loss: 0.9472 - val_accuracy: 0.6690\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.9682 - accuracy: 0.6576 - val_loss: 0.9329 - val_accuracy: 0.6743\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.9555 - accuracy: 0.6627 - val_loss: 0.9301 - val_accuracy: 0.6776\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.9445 - accuracy: 0.6691 - val_loss: 0.9274 - val_accuracy: 0.6760\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.9336 - accuracy: 0.6708 - val_loss: 0.9123 - val_accuracy: 0.6799\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.9207 - accuracy: 0.6754 - val_loss: 0.9142 - val_accuracy: 0.6828\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 1s 27ms/step - loss: 0.9172 - accuracy: 0.6787 - val_loss: 0.8965 - val_accuracy: 0.6892\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.9057 - accuracy: 0.6810 - val_loss: 0.8953 - val_accuracy: 0.6891\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.8900 - accuracy: 0.6846 - val_loss: 0.8851 - val_accuracy: 0.6932\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 1s 26ms/step - loss: 0.8765 - accuracy: 0.6926 - val_loss: 0.9056 - val_accuracy: 0.6867\n",
            "Test loss: 0.905562698841095\n",
            "Test accuracy: 0.6866999864578247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edC5E0jI_UP4"
      },
      "source": [
        " **Q:** Does that improve performance? \n",
        "\n",
        " **A:** Yes! Increase the accuracy by 5.45% to **68.66%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GeSmWuoEtCb"
      },
      "source": [
        "###model_4\n",
        "(which uses the same architecture from the MNIST network)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gi-SE65FHC2",
        "outputId": "92755c18-8bfe-418a-c12e-64cd784d2340"
      },
      "source": [
        "model_4 = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=x_train.shape[1:]),\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_4.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                23050     \n",
            "=================================================================\n",
            "Total params: 42,442\n",
            "Trainable params: 42,442\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3WuydgOH45F",
        "outputId": "3eeed7f2-76e0-4194-dc25-15e3062e5d8a"
      },
      "source": [
        "# initiate Adam optimizer like in the first example\n",
        "model_4.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_4.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=num_epochs,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "score = model_4.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 2s 34ms/step - loss: 2.1571 - accuracy: 0.1982 - val_loss: 1.7121 - val_accuracy: 0.3906\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 1s 29ms/step - loss: 1.6785 - accuracy: 0.3996 - val_loss: 1.5128 - val_accuracy: 0.4697\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 1.5186 - accuracy: 0.4569 - val_loss: 1.4199 - val_accuracy: 0.4945\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 1s 29ms/step - loss: 1.4305 - accuracy: 0.4913 - val_loss: 1.3515 - val_accuracy: 0.5281\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 1s 29ms/step - loss: 1.3794 - accuracy: 0.5104 - val_loss: 1.3136 - val_accuracy: 0.5416\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 1.3291 - accuracy: 0.5355 - val_loss: 1.2777 - val_accuracy: 0.5581\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 1s 29ms/step - loss: 1.2988 - accuracy: 0.5446 - val_loss: 1.2352 - val_accuracy: 0.5728\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 1.2636 - accuracy: 0.5589 - val_loss: 1.2041 - val_accuracy: 0.5857\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 1s 30ms/step - loss: 1.2442 - accuracy: 0.5657 - val_loss: 1.1819 - val_accuracy: 0.5908\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 1s 29ms/step - loss: 1.2134 - accuracy: 0.5788 - val_loss: 1.1566 - val_accuracy: 0.6016\n",
            "Test loss: 1.1566407680511475\n",
            "Test accuracy: 0.6015999913215637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32FQFAL5FDvh"
      },
      "source": [
        "**Q:** Does that work better?\n",
        "\n",
        "**A:** No! the accuracy is lower - **60.02%**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NylhFKNuJkSo"
      },
      "source": [
        "###model_5 \n",
        "\n",
        "(Intel suggestion:\n",
        "Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification , with **100 epochs**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVwEobKBJ1Vl",
        "outputId": "5b92a498-2503-4b37-d312-571d2bf5e071"
      },
      "source": [
        "# Let's build a CNN using Keras' Sequential capabilities\n",
        "\n",
        "model_5 = Sequential()\n",
        "\n",
        "\n",
        "## 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_5.add(Conv2D(32, (5, 5), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model_5.add(Activation('relu'))\n",
        "\n",
        "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_5.add(Conv2D(32, (5, 5), padding='same')) \n",
        "model_5.add(Activation('relu'))\n",
        "\n",
        "## 2x2 max pooling reduces to 3 x 3 x 32\n",
        "model_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_5.add(Dropout(0.25))\n",
        "\n",
        "## 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_5.add(Conv2D(32, (5, 5), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model_5.add(Activation('relu'))\n",
        "\n",
        "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
        "model_5.add(Conv2D(32, (5, 5), padding='same')) \n",
        "model_5.add(Activation('relu'))\n",
        "\n",
        "## 2x2 max pooling reduces to 3 x 3 x 32\n",
        "model_5.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_5.add(Dropout(0.25))\n",
        "\n",
        "## Flatten turns 3x3x32 into 288x1\n",
        "model_5.add(Flatten())\n",
        "model_5.add(Dense(512))\n",
        "model_5.add(Activation('relu'))\n",
        "model_5.add(Dropout(0.5))\n",
        "model_5.add(Dense(num_classes))\n",
        "model_5.add(Activation('softmax'))\n",
        "\n",
        "model_5.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_30 (Conv2D)           (None, 32, 32, 32)        2432      \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 32, 32, 32)        25632     \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 16, 16, 32)        25632     \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 16, 16, 32)        25632     \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 1,133,546\n",
            "Trainable params: 1,133,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSjK9GYhKXlv",
        "outputId": "fb59b2f8-2919-436e-f3bf-d41f26254c98"
      },
      "source": [
        "# initiate Adam optimizer like in the first example\n",
        "model_5.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_5.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=100,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "score = model_5.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "49/49 [==============================] - 6s 102ms/step - loss: 0.8356 - accuracy: 0.7664 - val_loss: 0.6957 - val_accuracy: 0.7761\n",
            "Epoch 2/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.3238 - accuracy: 0.8852 - val_loss: 0.6902 - val_accuracy: 0.7906\n",
            "Epoch 3/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.2645 - accuracy: 0.9068 - val_loss: 0.7073 - val_accuracy: 0.7898\n",
            "Epoch 4/100\n",
            "49/49 [==============================] - 5s 100ms/step - loss: 0.2551 - accuracy: 0.9075 - val_loss: 0.6969 - val_accuracy: 0.7897\n",
            "Epoch 5/100\n",
            "49/49 [==============================] - 5s 100ms/step - loss: 0.2483 - accuracy: 0.9076 - val_loss: 0.7171 - val_accuracy: 0.7882\n",
            "Epoch 6/100\n",
            "49/49 [==============================] - 5s 101ms/step - loss: 0.2560 - accuracy: 0.9080 - val_loss: 0.7155 - val_accuracy: 0.7904\n",
            "Epoch 7/100\n",
            "49/49 [==============================] - 5s 100ms/step - loss: 0.2400 - accuracy: 0.9141 - val_loss: 0.7182 - val_accuracy: 0.7875\n",
            "Epoch 8/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.2440 - accuracy: 0.9135 - val_loss: 0.7207 - val_accuracy: 0.7884\n",
            "Epoch 9/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.2424 - accuracy: 0.9144 - val_loss: 0.7156 - val_accuracy: 0.7867\n",
            "Epoch 10/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2479 - accuracy: 0.9100 - val_loss: 0.7285 - val_accuracy: 0.7883\n",
            "Epoch 11/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2406 - accuracy: 0.9135 - val_loss: 0.7084 - val_accuracy: 0.7925\n",
            "Epoch 12/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2359 - accuracy: 0.9146 - val_loss: 0.7102 - val_accuracy: 0.7862\n",
            "Epoch 13/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2382 - accuracy: 0.9144 - val_loss: 0.7367 - val_accuracy: 0.7869\n",
            "Epoch 14/100\n",
            "49/49 [==============================] - 5s 97ms/step - loss: 0.2332 - accuracy: 0.9152 - val_loss: 0.7208 - val_accuracy: 0.7913\n",
            "Epoch 15/100\n",
            "49/49 [==============================] - 5s 97ms/step - loss: 0.2239 - accuracy: 0.9212 - val_loss: 0.7243 - val_accuracy: 0.7910\n",
            "Epoch 16/100\n",
            "49/49 [==============================] - 5s 97ms/step - loss: 0.2303 - accuracy: 0.9184 - val_loss: 0.7344 - val_accuracy: 0.7889\n",
            "Epoch 17/100\n",
            "49/49 [==============================] - 5s 97ms/step - loss: 0.2235 - accuracy: 0.9194 - val_loss: 0.7705 - val_accuracy: 0.7839\n",
            "Epoch 18/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2336 - accuracy: 0.9184 - val_loss: 0.7515 - val_accuracy: 0.7885\n",
            "Epoch 19/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2225 - accuracy: 0.9207 - val_loss: 0.7634 - val_accuracy: 0.7849\n",
            "Epoch 20/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2229 - accuracy: 0.9203 - val_loss: 0.7457 - val_accuracy: 0.7908\n",
            "Epoch 21/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2125 - accuracy: 0.9242 - val_loss: 0.7257 - val_accuracy: 0.7893\n",
            "Epoch 22/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.2177 - accuracy: 0.9224 - val_loss: 0.7836 - val_accuracy: 0.7806\n",
            "Epoch 23/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.2176 - accuracy: 0.9213 - val_loss: 0.7685 - val_accuracy: 0.7881\n",
            "Epoch 24/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.2093 - accuracy: 0.9255 - val_loss: 0.7538 - val_accuracy: 0.7883\n",
            "Epoch 25/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.2117 - accuracy: 0.9249 - val_loss: 0.7566 - val_accuracy: 0.7901\n",
            "Epoch 26/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.2017 - accuracy: 0.9267 - val_loss: 0.7368 - val_accuracy: 0.7886\n",
            "Epoch 27/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2048 - accuracy: 0.9278 - val_loss: 0.7603 - val_accuracy: 0.7914\n",
            "Epoch 28/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2036 - accuracy: 0.9282 - val_loss: 0.7796 - val_accuracy: 0.7861\n",
            "Epoch 29/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1984 - accuracy: 0.9291 - val_loss: 0.7828 - val_accuracy: 0.7923\n",
            "Epoch 30/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1953 - accuracy: 0.9296 - val_loss: 0.7655 - val_accuracy: 0.7866\n",
            "Epoch 31/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.2092 - accuracy: 0.9267 - val_loss: 0.7536 - val_accuracy: 0.7888\n",
            "Epoch 32/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1957 - accuracy: 0.9301 - val_loss: 0.7630 - val_accuracy: 0.7905\n",
            "Epoch 33/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1910 - accuracy: 0.9320 - val_loss: 0.7469 - val_accuracy: 0.7903\n",
            "Epoch 34/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1924 - accuracy: 0.9306 - val_loss: 0.7651 - val_accuracy: 0.7891\n",
            "Epoch 35/100\n",
            "49/49 [==============================] - 5s 97ms/step - loss: 0.1930 - accuracy: 0.9321 - val_loss: 0.7690 - val_accuracy: 0.7892\n",
            "Epoch 36/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1906 - accuracy: 0.9316 - val_loss: 0.8051 - val_accuracy: 0.7841\n",
            "Epoch 37/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1956 - accuracy: 0.9299 - val_loss: 0.7559 - val_accuracy: 0.7900\n",
            "Epoch 38/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1843 - accuracy: 0.9346 - val_loss: 0.7718 - val_accuracy: 0.7868\n",
            "Epoch 39/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1866 - accuracy: 0.9341 - val_loss: 0.7855 - val_accuracy: 0.7855\n",
            "Epoch 40/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1876 - accuracy: 0.9318 - val_loss: 0.7831 - val_accuracy: 0.7909\n",
            "Epoch 41/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1883 - accuracy: 0.9346 - val_loss: 0.8114 - val_accuracy: 0.7903\n",
            "Epoch 42/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1906 - accuracy: 0.9319 - val_loss: 0.7799 - val_accuracy: 0.7939\n",
            "Epoch 43/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1831 - accuracy: 0.9356 - val_loss: 0.7760 - val_accuracy: 0.7905\n",
            "Epoch 44/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1745 - accuracy: 0.9374 - val_loss: 0.7650 - val_accuracy: 0.7902\n",
            "Epoch 45/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1746 - accuracy: 0.9375 - val_loss: 0.7784 - val_accuracy: 0.7917\n",
            "Epoch 46/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1685 - accuracy: 0.9392 - val_loss: 0.7699 - val_accuracy: 0.7899\n",
            "Epoch 47/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1800 - accuracy: 0.9353 - val_loss: 0.7924 - val_accuracy: 0.7915\n",
            "Epoch 48/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1755 - accuracy: 0.9381 - val_loss: 0.8179 - val_accuracy: 0.7897\n",
            "Epoch 49/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1939 - accuracy: 0.9317 - val_loss: 0.7973 - val_accuracy: 0.7914\n",
            "Epoch 50/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1728 - accuracy: 0.9393 - val_loss: 0.7897 - val_accuracy: 0.7915\n",
            "Epoch 51/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1740 - accuracy: 0.9387 - val_loss: 0.7832 - val_accuracy: 0.7932\n",
            "Epoch 52/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1707 - accuracy: 0.9391 - val_loss: 0.7981 - val_accuracy: 0.7922\n",
            "Epoch 53/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1672 - accuracy: 0.9409 - val_loss: 0.7860 - val_accuracy: 0.7874\n",
            "Epoch 54/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1731 - accuracy: 0.9396 - val_loss: 0.7835 - val_accuracy: 0.7950\n",
            "Epoch 55/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1660 - accuracy: 0.9420 - val_loss: 0.8042 - val_accuracy: 0.7928\n",
            "Epoch 56/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1617 - accuracy: 0.9423 - val_loss: 0.7923 - val_accuracy: 0.7903\n",
            "Epoch 57/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1810 - accuracy: 0.9371 - val_loss: 0.7683 - val_accuracy: 0.7947\n",
            "Epoch 58/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1668 - accuracy: 0.9414 - val_loss: 0.8210 - val_accuracy: 0.7892\n",
            "Epoch 59/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1655 - accuracy: 0.9407 - val_loss: 0.8058 - val_accuracy: 0.7918\n",
            "Epoch 60/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1593 - accuracy: 0.9418 - val_loss: 0.7865 - val_accuracy: 0.7927\n",
            "Epoch 61/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1582 - accuracy: 0.9440 - val_loss: 0.7809 - val_accuracy: 0.7948\n",
            "Epoch 62/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1608 - accuracy: 0.9428 - val_loss: 0.8130 - val_accuracy: 0.7924\n",
            "Epoch 63/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1577 - accuracy: 0.9451 - val_loss: 0.8147 - val_accuracy: 0.7937\n",
            "Epoch 64/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1583 - accuracy: 0.9433 - val_loss: 0.7890 - val_accuracy: 0.7943\n",
            "Epoch 65/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1583 - accuracy: 0.9447 - val_loss: 0.8395 - val_accuracy: 0.7938\n",
            "Epoch 66/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1550 - accuracy: 0.9457 - val_loss: 0.8158 - val_accuracy: 0.7890\n",
            "Epoch 67/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1603 - accuracy: 0.9443 - val_loss: 0.8105 - val_accuracy: 0.7937\n",
            "Epoch 68/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1651 - accuracy: 0.9417 - val_loss: 0.7779 - val_accuracy: 0.7973\n",
            "Epoch 69/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1554 - accuracy: 0.9443 - val_loss: 0.7920 - val_accuracy: 0.7901\n",
            "Epoch 70/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1471 - accuracy: 0.9473 - val_loss: 0.8014 - val_accuracy: 0.7913\n",
            "Epoch 71/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1509 - accuracy: 0.9469 - val_loss: 0.8146 - val_accuracy: 0.7926\n",
            "Epoch 72/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1517 - accuracy: 0.9466 - val_loss: 0.7738 - val_accuracy: 0.7938\n",
            "Epoch 73/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1468 - accuracy: 0.9473 - val_loss: 0.8096 - val_accuracy: 0.7948\n",
            "Epoch 74/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1567 - accuracy: 0.9455 - val_loss: 0.8140 - val_accuracy: 0.7918\n",
            "Epoch 75/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1538 - accuracy: 0.9450 - val_loss: 0.8154 - val_accuracy: 0.7941\n",
            "Epoch 76/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1542 - accuracy: 0.9453 - val_loss: 0.8167 - val_accuracy: 0.7908\n",
            "Epoch 77/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1493 - accuracy: 0.9449 - val_loss: 0.8458 - val_accuracy: 0.7938\n",
            "Epoch 78/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1520 - accuracy: 0.9470 - val_loss: 0.8089 - val_accuracy: 0.7942\n",
            "Epoch 79/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1546 - accuracy: 0.9458 - val_loss: 0.8391 - val_accuracy: 0.7938\n",
            "Epoch 80/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1579 - accuracy: 0.9453 - val_loss: 0.8609 - val_accuracy: 0.7935\n",
            "Epoch 81/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1527 - accuracy: 0.9449 - val_loss: 0.8220 - val_accuracy: 0.7874\n",
            "Epoch 82/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1617 - accuracy: 0.9428 - val_loss: 0.8257 - val_accuracy: 0.7925\n",
            "Epoch 83/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1568 - accuracy: 0.9444 - val_loss: 0.8517 - val_accuracy: 0.7935\n",
            "Epoch 84/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1488 - accuracy: 0.9481 - val_loss: 0.8338 - val_accuracy: 0.7919\n",
            "Epoch 85/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1555 - accuracy: 0.9455 - val_loss: 0.8046 - val_accuracy: 0.7964\n",
            "Epoch 86/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1443 - accuracy: 0.9482 - val_loss: 0.8433 - val_accuracy: 0.7914\n",
            "Epoch 87/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1510 - accuracy: 0.9470 - val_loss: 0.8332 - val_accuracy: 0.7930\n",
            "Epoch 88/100\n",
            "49/49 [==============================] - 5s 99ms/step - loss: 0.1471 - accuracy: 0.9486 - val_loss: 0.8118 - val_accuracy: 0.7903\n",
            "Epoch 89/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1445 - accuracy: 0.9501 - val_loss: 0.8262 - val_accuracy: 0.7930\n",
            "Epoch 90/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1460 - accuracy: 0.9482 - val_loss: 0.8007 - val_accuracy: 0.7958\n",
            "Epoch 91/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1417 - accuracy: 0.9493 - val_loss: 0.8101 - val_accuracy: 0.7943\n",
            "Epoch 92/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1370 - accuracy: 0.9521 - val_loss: 0.8356 - val_accuracy: 0.7931\n",
            "Epoch 93/100\n",
            "49/49 [==============================] - 5s 97ms/step - loss: 0.1437 - accuracy: 0.9499 - val_loss: 0.8446 - val_accuracy: 0.7939\n",
            "Epoch 94/100\n",
            "49/49 [==============================] - 5s 97ms/step - loss: 0.1451 - accuracy: 0.9504 - val_loss: 0.8220 - val_accuracy: 0.7887\n",
            "Epoch 95/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1380 - accuracy: 0.9520 - val_loss: 0.8440 - val_accuracy: 0.7942\n",
            "Epoch 96/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1386 - accuracy: 0.9522 - val_loss: 0.8151 - val_accuracy: 0.7938\n",
            "Epoch 97/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1418 - accuracy: 0.9505 - val_loss: 0.8456 - val_accuracy: 0.7942\n",
            "Epoch 98/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1393 - accuracy: 0.9509 - val_loss: 0.8482 - val_accuracy: 0.7895\n",
            "Epoch 99/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1390 - accuracy: 0.9526 - val_loss: 0.8562 - val_accuracy: 0.7916\n",
            "Epoch 100/100\n",
            "49/49 [==============================] - 5s 98ms/step - loss: 0.1341 - accuracy: 0.9528 - val_loss: 0.8649 - val_accuracy: 0.7871\n",
            "Test loss: 0.8648883700370789\n",
            "Test accuracy: 0.7871000170707703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI84bjH5PJLn"
      },
      "source": [
        "Intel model (with 100 epochs) results: **78.71%** accuracy! **(>75%)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrH7N1HbOiYv"
      },
      "source": [
        "### model_6\n",
        "We wanted to try to pass the 80% accuracy and built the following model that based on [this](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/) article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gywON8GONHYv",
        "outputId": "609b4464-39b1-498a-e00a-d9b853cbdbdb"
      },
      "source": [
        "model_6 = Sequential()\n",
        "model_6.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "model_6.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model_6.add(MaxPooling2D((2, 2)))\n",
        "model_6.add(Dropout(0.2))\n",
        "model_6.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model_6.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model_6.add(MaxPooling2D((2, 2)))\n",
        "model_6.add(Dropout(0.2))\n",
        "model_6.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model_6.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model_6.add(MaxPooling2D((2, 2)))\n",
        "model_6.add(Dropout(0.2))\n",
        "model_6.add(Flatten())\n",
        "model_6.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model_6.add(Dropout(0.2))\n",
        "model_6.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "model_6.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_6.fit(x_train, y_train,\n",
        "              batch_size=64,\n",
        "              epochs=100,\n",
        "              validation_data=(x_test, y_test),\n",
        "              shuffle=True)\n",
        "\n",
        "score = model_6.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "782/782 [==============================] - 8s 9ms/step - loss: 2.2325 - accuracy: 0.1749 - val_loss: 1.6972 - val_accuracy: 0.4033\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.7132 - accuracy: 0.3646 - val_loss: 1.4981 - val_accuracy: 0.4490\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.5402 - accuracy: 0.4364 - val_loss: 1.3584 - val_accuracy: 0.5192\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.4233 - accuracy: 0.4823 - val_loss: 1.2580 - val_accuracy: 0.5502\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.3381 - accuracy: 0.5159 - val_loss: 1.2636 - val_accuracy: 0.5502\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.2725 - accuracy: 0.5396 - val_loss: 1.1303 - val_accuracy: 0.6008\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.2022 - accuracy: 0.5674 - val_loss: 1.1812 - val_accuracy: 0.5791\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 1.1485 - accuracy: 0.5892 - val_loss: 1.0153 - val_accuracy: 0.6412\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.0813 - accuracy: 0.6112 - val_loss: 0.9762 - val_accuracy: 0.6525\n",
            "Epoch 10/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.0413 - accuracy: 0.6301 - val_loss: 0.9949 - val_accuracy: 0.6475\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 1.0000 - accuracy: 0.6391 - val_loss: 0.8932 - val_accuracy: 0.6832\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.9565 - accuracy: 0.6618 - val_loss: 0.8580 - val_accuracy: 0.6957\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.9291 - accuracy: 0.6666 - val_loss: 0.8628 - val_accuracy: 0.6902\n",
            "Epoch 14/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.8963 - accuracy: 0.6804 - val_loss: 0.8050 - val_accuracy: 0.7196\n",
            "Epoch 15/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.8819 - accuracy: 0.6848 - val_loss: 0.7998 - val_accuracy: 0.7179\n",
            "Epoch 16/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.8439 - accuracy: 0.7009 - val_loss: 0.7763 - val_accuracy: 0.7246\n",
            "Epoch 17/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.8181 - accuracy: 0.7092 - val_loss: 0.7627 - val_accuracy: 0.7353\n",
            "Epoch 18/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.8058 - accuracy: 0.7150 - val_loss: 0.7495 - val_accuracy: 0.7379\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.7890 - accuracy: 0.7210 - val_loss: 0.7591 - val_accuracy: 0.7339\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.7707 - accuracy: 0.7266 - val_loss: 0.7164 - val_accuracy: 0.7464\n",
            "Epoch 21/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.7588 - accuracy: 0.7311 - val_loss: 0.7430 - val_accuracy: 0.7384\n",
            "Epoch 22/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.7504 - accuracy: 0.7367 - val_loss: 0.7097 - val_accuracy: 0.7513\n",
            "Epoch 23/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.7283 - accuracy: 0.7406 - val_loss: 0.7226 - val_accuracy: 0.7438\n",
            "Epoch 24/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.7127 - accuracy: 0.7482 - val_loss: 0.6913 - val_accuracy: 0.7602\n",
            "Epoch 25/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6970 - accuracy: 0.7541 - val_loss: 0.6676 - val_accuracy: 0.7646\n",
            "Epoch 26/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.6706 - accuracy: 0.7622 - val_loss: 0.6659 - val_accuracy: 0.7665\n",
            "Epoch 27/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6666 - accuracy: 0.7655 - val_loss: 0.6513 - val_accuracy: 0.7740\n",
            "Epoch 28/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6615 - accuracy: 0.7675 - val_loss: 0.6634 - val_accuracy: 0.7665\n",
            "Epoch 29/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6405 - accuracy: 0.7744 - val_loss: 0.6390 - val_accuracy: 0.7721\n",
            "Epoch 30/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6218 - accuracy: 0.7802 - val_loss: 0.6476 - val_accuracy: 0.7750\n",
            "Epoch 31/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6186 - accuracy: 0.7846 - val_loss: 0.6512 - val_accuracy: 0.7771\n",
            "Epoch 32/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.6111 - accuracy: 0.7859 - val_loss: 0.6258 - val_accuracy: 0.7801\n",
            "Epoch 33/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5958 - accuracy: 0.7901 - val_loss: 0.6295 - val_accuracy: 0.7777\n",
            "Epoch 34/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5864 - accuracy: 0.7910 - val_loss: 0.6343 - val_accuracy: 0.7789\n",
            "Epoch 35/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5959 - accuracy: 0.7913 - val_loss: 0.6131 - val_accuracy: 0.7872\n",
            "Epoch 36/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5739 - accuracy: 0.7952 - val_loss: 0.5888 - val_accuracy: 0.7914\n",
            "Epoch 37/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5605 - accuracy: 0.8048 - val_loss: 0.5924 - val_accuracy: 0.7935\n",
            "Epoch 38/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5543 - accuracy: 0.8021 - val_loss: 0.5804 - val_accuracy: 0.7958\n",
            "Epoch 39/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5395 - accuracy: 0.8092 - val_loss: 0.6135 - val_accuracy: 0.7863\n",
            "Epoch 40/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.5314 - accuracy: 0.8137 - val_loss: 0.5900 - val_accuracy: 0.7964\n",
            "Epoch 41/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5206 - accuracy: 0.8164 - val_loss: 0.5746 - val_accuracy: 0.7984\n",
            "Epoch 42/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5215 - accuracy: 0.8134 - val_loss: 0.5763 - val_accuracy: 0.7982\n",
            "Epoch 43/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5106 - accuracy: 0.8179 - val_loss: 0.5946 - val_accuracy: 0.7918\n",
            "Epoch 44/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.5130 - accuracy: 0.8178 - val_loss: 0.5753 - val_accuracy: 0.7999\n",
            "Epoch 45/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4975 - accuracy: 0.8238 - val_loss: 0.5758 - val_accuracy: 0.8025\n",
            "Epoch 46/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4892 - accuracy: 0.8256 - val_loss: 0.5755 - val_accuracy: 0.8014\n",
            "Epoch 47/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4827 - accuracy: 0.8302 - val_loss: 0.5649 - val_accuracy: 0.8059\n",
            "Epoch 48/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4765 - accuracy: 0.8331 - val_loss: 0.5622 - val_accuracy: 0.8034\n",
            "Epoch 49/100\n",
            "782/782 [==============================] - 6s 8ms/step - loss: 0.4667 - accuracy: 0.8356 - val_loss: 0.5697 - val_accuracy: 0.8044\n",
            "Epoch 50/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4638 - accuracy: 0.8347 - val_loss: 0.5478 - val_accuracy: 0.8070\n",
            "Epoch 51/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4540 - accuracy: 0.8414 - val_loss: 0.5718 - val_accuracy: 0.8023\n",
            "Epoch 52/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4537 - accuracy: 0.8396 - val_loss: 0.5463 - val_accuracy: 0.8108\n",
            "Epoch 53/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4345 - accuracy: 0.8440 - val_loss: 0.5479 - val_accuracy: 0.8094\n",
            "Epoch 54/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4449 - accuracy: 0.8439 - val_loss: 0.5855 - val_accuracy: 0.8030\n",
            "Epoch 55/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4284 - accuracy: 0.8482 - val_loss: 0.5448 - val_accuracy: 0.8121\n",
            "Epoch 56/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4285 - accuracy: 0.8482 - val_loss: 0.5617 - val_accuracy: 0.8129\n",
            "Epoch 57/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4260 - accuracy: 0.8477 - val_loss: 0.5513 - val_accuracy: 0.8144\n",
            "Epoch 58/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4185 - accuracy: 0.8484 - val_loss: 0.5378 - val_accuracy: 0.8159\n",
            "Epoch 59/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4042 - accuracy: 0.8573 - val_loss: 0.5519 - val_accuracy: 0.8152\n",
            "Epoch 60/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.4029 - accuracy: 0.8551 - val_loss: 0.5602 - val_accuracy: 0.8121\n",
            "Epoch 61/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3970 - accuracy: 0.8607 - val_loss: 0.5399 - val_accuracy: 0.8162\n",
            "Epoch 62/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3855 - accuracy: 0.8619 - val_loss: 0.5447 - val_accuracy: 0.8158\n",
            "Epoch 63/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3885 - accuracy: 0.8613 - val_loss: 0.5329 - val_accuracy: 0.8216\n",
            "Epoch 64/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3818 - accuracy: 0.8605 - val_loss: 0.5405 - val_accuracy: 0.8180\n",
            "Epoch 65/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3752 - accuracy: 0.8679 - val_loss: 0.5375 - val_accuracy: 0.8214\n",
            "Epoch 66/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3687 - accuracy: 0.8689 - val_loss: 0.5344 - val_accuracy: 0.8226\n",
            "Epoch 67/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3649 - accuracy: 0.8688 - val_loss: 0.5469 - val_accuracy: 0.8185\n",
            "Epoch 68/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3596 - accuracy: 0.8689 - val_loss: 0.5596 - val_accuracy: 0.8198\n",
            "Epoch 69/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3606 - accuracy: 0.8695 - val_loss: 0.5358 - val_accuracy: 0.8216\n",
            "Epoch 70/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3519 - accuracy: 0.8745 - val_loss: 0.5438 - val_accuracy: 0.8223\n",
            "Epoch 71/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3497 - accuracy: 0.8771 - val_loss: 0.5502 - val_accuracy: 0.8182\n",
            "Epoch 72/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3455 - accuracy: 0.8752 - val_loss: 0.5472 - val_accuracy: 0.8183\n",
            "Epoch 73/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3374 - accuracy: 0.8786 - val_loss: 0.5337 - val_accuracy: 0.8265\n",
            "Epoch 74/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3275 - accuracy: 0.8817 - val_loss: 0.5448 - val_accuracy: 0.8211\n",
            "Epoch 75/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3307 - accuracy: 0.8799 - val_loss: 0.5435 - val_accuracy: 0.8231\n",
            "Epoch 76/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3173 - accuracy: 0.8857 - val_loss: 0.5455 - val_accuracy: 0.8270\n",
            "Epoch 77/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3157 - accuracy: 0.8869 - val_loss: 0.5620 - val_accuracy: 0.8186\n",
            "Epoch 78/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3127 - accuracy: 0.8882 - val_loss: 0.5398 - val_accuracy: 0.8268\n",
            "Epoch 79/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3102 - accuracy: 0.8897 - val_loss: 0.5719 - val_accuracy: 0.8203\n",
            "Epoch 80/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3071 - accuracy: 0.8893 - val_loss: 0.5263 - val_accuracy: 0.8320\n",
            "Epoch 81/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2986 - accuracy: 0.8904 - val_loss: 0.5458 - val_accuracy: 0.8251\n",
            "Epoch 82/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.3033 - accuracy: 0.8914 - val_loss: 0.5406 - val_accuracy: 0.8267\n",
            "Epoch 83/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2870 - accuracy: 0.8967 - val_loss: 0.5340 - val_accuracy: 0.8299\n",
            "Epoch 84/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2833 - accuracy: 0.8975 - val_loss: 0.5394 - val_accuracy: 0.8285\n",
            "Epoch 85/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2837 - accuracy: 0.8983 - val_loss: 0.5442 - val_accuracy: 0.8292\n",
            "Epoch 86/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2887 - accuracy: 0.8949 - val_loss: 0.5418 - val_accuracy: 0.8273\n",
            "Epoch 87/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2823 - accuracy: 0.8972 - val_loss: 0.5522 - val_accuracy: 0.8296\n",
            "Epoch 88/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2847 - accuracy: 0.8966 - val_loss: 0.5720 - val_accuracy: 0.8252\n",
            "Epoch 89/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2735 - accuracy: 0.9014 - val_loss: 0.5583 - val_accuracy: 0.8271\n",
            "Epoch 90/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2747 - accuracy: 0.9013 - val_loss: 0.5729 - val_accuracy: 0.8279\n",
            "Epoch 91/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2637 - accuracy: 0.9032 - val_loss: 0.5488 - val_accuracy: 0.8281\n",
            "Epoch 92/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2653 - accuracy: 0.9023 - val_loss: 0.5534 - val_accuracy: 0.8326\n",
            "Epoch 93/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2539 - accuracy: 0.9095 - val_loss: 0.5503 - val_accuracy: 0.8351\n",
            "Epoch 94/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2525 - accuracy: 0.9084 - val_loss: 0.5864 - val_accuracy: 0.8258\n",
            "Epoch 95/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2491 - accuracy: 0.9098 - val_loss: 0.5634 - val_accuracy: 0.8317\n",
            "Epoch 96/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2525 - accuracy: 0.9083 - val_loss: 0.5864 - val_accuracy: 0.8284\n",
            "Epoch 97/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2498 - accuracy: 0.9075 - val_loss: 0.5609 - val_accuracy: 0.8316\n",
            "Epoch 98/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2465 - accuracy: 0.9102 - val_loss: 0.5571 - val_accuracy: 0.8323\n",
            "Epoch 99/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2436 - accuracy: 0.9125 - val_loss: 0.5846 - val_accuracy: 0.8262\n",
            "Epoch 100/100\n",
            "782/782 [==============================] - 7s 8ms/step - loss: 0.2374 - accuracy: 0.9126 - val_loss: 0.5609 - val_accuracy: 0.8311\n",
            "Test loss: 0.5608813762664795\n",
            "Test accuracy: 0.8310999870300293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKbGD7dFQb5C"
      },
      "source": [
        "### We got about **83%** accuracy"
      ]
    }
  ]
}